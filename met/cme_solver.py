from scipy.stats import poisson, zipf
import os
import numpy as np
import torch
from .hyperparameters import default_dtype_torch
os.environ["KMP_DUPLICATE_LIB_OK"] = "True"


def delta_function(args, X, Y):
    """
    manually defined delta function
    :param args: global arguments
    :param X: input samples, shape (BS, N)
    :param Y: initial poisson parameters, the initial state, shape (BS, N),
                prob(Y) = 1, prob(not Y) = 0
    :return: delta probability, shape (BS, N)
    """
    N = args.num_species
    U = args.state_upper_bound
    BS = args.batch_size
    # not too large to make a delta distribution
    # not too small to avoid numerical instability of the perfect delta distribution
    factor = 0.01
    prob = torch.zeros_like(X, dtype=default_dtype_torch, device=args.device)
    if args.constrains[0] == 0:
        # args.constrain_states->N
        args.constrains = U * np.ones(N, dtype=int)
    if len(X.shape) == 2:
        small_prob = factor / (args.constrains - 1)
        small_prob = small_prob.reshape(1, -1).repeat(BS, axis=0)
        small_prob = torch.tensor(small_prob, dtype=default_dtype_torch).to(args.device)
    elif len(X.shape) == 3:
        small_prob = factor / (args.constrains - 1)
        small_prob = small_prob.reshape(1, -1, 1).repeat(BS, axis=0).repeat(X.shape[2], axis=2)
        small_prob = torch.tensor(small_prob, dtype=default_dtype_torch).to(args.device)
    else:
        raise ValueError(f"X's is 2 or 3 dimension, but input is {len(X.shape)} dimension")

    prob[X - Y == 0] = torch.tensor(1 - factor, dtype=default_dtype_torch).to(args.device)
    prob[X - Y != 0] = small_prob[X - Y != 0]
    return prob


def cme_state_joint_prob(
        sample,
        args,
        cme_step,
        train_epoch,
        net,
        reaction_mat_left,
        transition_matrix,
        rate_parameters,
        delta_t,
        propensity_function,
):
    """
    BS: batch size
    N: number of species
    M: number of reactions
    U: upper bound for species
    Calculate the state joint probability given state joint probability calculated from net with token
    The net should be a deep copy of the current net to avoid disrupting the weights during sampling

    :param sample: sample generated by under trained net, shape (BS, N)
    :param args: global arguments
    :param cme_step: current step of cme
    :param train_epoch: current training epoch
    :param net: deep copied net for calculating the joint probability of samples
                **warning**: do not pass the under trained net
    :param reaction_mat_left: used for calculating the propensities, shape (N, M)
    :param transition_matrix: locating the related states to samples, shape (N, M)
    :param rate_parameters: rate parameters, shape (M, )
    :param delta_t: input possible dt for cme transition probability calculation
    :param propensity_function: function for computing propensities
    :return: joint probability of samples given previous joint state probability
    """

    N = args.num_species
    M = args.num_reactions
    U = args.state_upper_bound
    BS = args.batch_size
    prompt = None
    if args.net == "rnn" or args.net == "transformer":
        sample_1d = (sample.view(-1, N))  # sample has size batch-size x systemSize
    elif args.net == "met":
        sample_1d = sample[:, -N:]
        prompt = sample[:, :-N]
    else:
        raise ValueError("Unknown net type.")
    # All possible configurations reacted to the sampled state: BatchSize x SystemSize  X Reactions
    sample_neighbor_1d = sample_1d.repeat(transition_matrix.shape[1], 1, 1).permute(1, 2, 0)
    # States that transit into the sampled states by each of the reation
    sample_neighbor_1d_Win = sample_neighbor_1d - transition_matrix
    up_boundary = U
    if args.conservation > 1 and args.Sites == 1:
        up_boundary = args.conservation - 1
    if args.constrains[0] > 0:
        up_boundary = torch.tensor(args.constrains, dtype=sample_neighbor_1d_Win.dtype).to(args.device)
        up_boundary = up_boundary.view(-1, 1).repeat(sample_neighbor_1d.shape[0], 1, sample_neighbor_1d.shape[2])
        sample_neighbor_1d_Win[sample_neighbor_1d_Win >= up_boundary] = up_boundary[sample_neighbor_1d_Win >= up_boundary]  # -1
    neg_state_idx_in = sample_neighbor_1d_Win < 0
    sample_neighbor_1d_Win[neg_state_idx_in] = 0  # Make the negative number of species to zero
    out_bound_idx_in = sample_neighbor_1d_Win >= up_boundary
    sample_neighbor_1d_Wout = sample_neighbor_1d + transition_matrix  # States that transit out from the sampled states by each of the reation
    if args.constrains[0] > 0:
        sample_neighbor_1d_Win[out_bound_idx_in] = up_boundary[out_bound_idx_in] - 1
        sample_neighbor_1d_Wout[sample_neighbor_1d_Wout >= up_boundary] = up_boundary[sample_neighbor_1d_Wout >= up_boundary]  # -1
    else:
        sample_neighbor_1d_Win[out_bound_idx_in] = up_boundary - 1
    neg_state_idx_out = sample_neighbor_1d_Wout < 0
    out_bound_idx_out = sample_neighbor_1d_Wout >= up_boundary

    # Propensity: Make product of chemical flux and reaction rates
    Win = sample_neighbor_1d_Win ** reaction_mat_left  # Reaction in-flux by [Species]^Stoichiometric
    # TODO: change to combinatorial number, for comb(5, 5) is very different than 5**5
    # Make the not-happen reaction with zero chemical species, giving zero flux when multiplied below
    Win[neg_state_idx_in] = 0
    Win[out_bound_idx_in] = 0
    # Check right boundary: done, reflecting boundary condition
    Wout = sample_neighbor_1d ** reaction_mat_left  # Reaction out-flux by [Species]^Stoichiometric
    Wout[neg_state_idx_out] = 0
    Wout[out_bound_idx_out] = 0

    propensity_in, propensity_out = propensity_function(
        Win,
        Wout,
        rate_parameters,
        sample_neighbor_1d_Win,
        sample_neighbor_1d,
        neg_state_idx_in,
        out_bound_idx_in,
        neg_state_idx_out,
        out_bound_idx_out
    )

    R = torch.sum(propensity_out, 1) # sum of out propensities for all reactions

    if cme_step == 0:  # initial distribution: depending on the system
        with torch.no_grad():
            if args.initial_distribution_type == 'delta':
                # Manually generate delta-distribution

                f = args.initial_distribution.repeat(BS, axis=0)
                f = delta_function(args, sample_1d.cpu(), f)
                f = torch.as_tensor(f, dtype=default_dtype_torch)
                f = f + args.epsilon
                f = f.to(args.device)
                f = torch.log(f)
                log_p_t = torch.sum(f, 1)

                f = args.initial_distribution.repeat(BS, axis=0)
                f = f.reshape(BS, N, 1)
                f = f.repeat(reaction_mat_left.shape[1], axis=2)
                f = delta_function(args, sample_neighbor_1d_Win.cpu(), f)
                f = torch.as_tensor(f, dtype=default_dtype_torch)
                f = f + args.epsilon
                f = f.to(args.device)
                f = torch.log(f)
                log_p_t_other = torch.sum(f, 1)

            elif args.initial_distribution_type == 'zipf':
                # #Zipf distribution to generate delta-distribution

                f = args.initial_distribution.repeat(BS, axis=0)
                f = zipf.pmf(sample_1d.cpu() - f, 6)
                f = torch.as_tensor(f, dtype=default_dtype_torch)
                f = f + args.epsilon
                f = f.to(args.device)
                f = torch.log(f)
                log_p_t = torch.sum(f, 1)

                f = args.initial_distribution.repeat(BS, axis=0)
                f = f.reshape(BS, N, 1)
                f = f.repeat(reaction_mat_left.shape[1], axis=2)
                f = zipf.pmf(sample_neighbor_1d_Win.cpu() - f, 6)
                f = torch.as_tensor(f, dtype=default_dtype_torch)
                f = f + args.epsilon
                f = f.to(args.device)
                f = torch.log(f)
                log_p_t_other = torch.sum(f, 1)

            elif args.initial_distribution_type == 'MM':
                f11 = poisson.pmf(k=sample_1d[:, :2].cpu(), mu=args.initial_distribution)  # substrate,#enzyme
                f13 = poisson.pmf(k=sample_1d[:, 2:].cpu(), mu=0.1)  # a complex, a product,
                f1 = np.concatenate((f11, f13), axis=1)
                f1 = torch.as_tensor(f1, dtype=default_dtype_torch)
                f1 = f1.to(args.device)
                f1 = torch.log(f1)
                log_p_t = torch.sum(f1, 1)

                f21 = poisson.pmf(k=sample_neighbor_1d_Win[:, :2, :].cpu(), mu=args.initial_distribution)
                f22 = poisson.pmf(k=sample_neighbor_1d_Win[:, 2:, :].cpu(), mu=0.1)
                f2 = np.concatenate((f21, f22), axis=1)
                f2 = torch.as_tensor(f2, dtype=default_dtype_torch)
                f2 = f2.to(args.device)
                f2 = torch.log(f2)
                log_p_t_other = torch.sum(f2, 1)

            else:
                if N == 1:
                    initial_d2 = args.initial_distribution.repeat(BS, axis=0).reshape(-1, 1)
                else:
                    initial_d2 = args.initial_distribution.repeat(BS, axis=0)

                f = poisson.pmf(k=sample_1d.cpu(), mu=initial_d2)
                f = torch.as_tensor(f, dtype=default_dtype_torch)
                f = f + args.epsilon
                f = f.to(args.device)
                f = torch.log(f)
                log_p_t = torch.sum(f, 1)

                f = args.initial_distribution.repeat(BS, axis=0)
                f = f.reshape(BS, N, 1)
                f = f.repeat(reaction_mat_left.shape[1], axis=2)
                f = poisson.pmf(k=sample_neighbor_1d_Win.cpu(), mu=f)
                f = torch.as_tensor(f, dtype=default_dtype_torch)
                f = f + args.epsilon
                f = f.to(args.device)
                f = torch.log(f)
                log_p_t_other = torch.sum(f, 1)

    if cme_step > 0:
        with torch.no_grad():
            if args.net == "met":
                pro_sam = torch.cat((prompt, sample_1d), dim=1)
                log_p_t = net.log_joint_prob(pro_sam).detach()
            elif args.net == "rnn" or args.net == "transformer":
                log_p_t = net.log_prob(sample).detach()
            temp = torch.transpose(sample_neighbor_1d_Win, 1, 2)
            temp = torch.reshape(temp, (-1, N))
            if args.net == "met":
                prompt = prompt.repeat(M, 1)
                pro_sam = torch.cat((prompt, temp), dim=1)
                log_p_t_other_temp = net.log_joint_prob(pro_sam).detach()
            elif args.net == "rnn" or args.net == "transformer":
                log_p_t_other_temp = net.log_prob(temp).detach()
            log_p_t_other = torch.reshape(log_p_t_other_temp, (-1, M))

    with torch.no_grad():
        if args.use_adaptive_cme_time:  # only at the first 10 epoch, and record it
            if train_epoch == 0:
                delta_t = args.cme_dt * args.adaptive_time_fold

            f2 = log_p_t_other - log_p_t.repeat(M, 1).t()
            f2 = torch.exp(f2)
            f2 = f2 * propensity_in
            f2 = torch.sum(f2, 1)
            f2 = 1 + (f2 - R) * delta_t

            if train_epoch <= 10:
                while torch.min(f2) < 0:
                    delta_t = delta_t / 2
                    f2 = log_p_t_other - log_p_t.repeat(M, 1).t()
                    f2 = torch.exp(f2)
                    f2 = f2 * propensity_in
                    f2 = torch.sum(f2, 1)
                    f2 = 1 + (f2 - R) * delta_t
                    if delta_t <= args.cme_dt:
                        print(f"reduce delta_t to {delta_t} at train step {train_epoch} cme step {cme_step}")
                        f2[f2 <= 0] = args.epsilon
                        break
            else:
                f2 = log_p_t_other - log_p_t.repeat(M, 1).t()
                f2 = torch.exp(f2)
                f2 = f2 * propensity_in
                f2 = torch.sum(f2, 1)
                f2 = 1 + (f2 - R) * delta_t
                f2[f2 <= 0] = args.epsilon
        else:
            if cme_step == 0:
                f2 = log_p_t_other - log_p_t.repeat(M, 1).t()
                f2 = torch.exp(f2)
                f2 = f2 * propensity_in
                f2 = torch.sum(f2, 1)
                delta_t = args.cme_dt / 10
                f2 = 1 + (f2 - R) * delta_t
            else:
                f2 = log_p_t_other - log_p_t.repeat(M, 1).t()
                f2 = torch.exp(f2)
                f2 = f2 * propensity_in
                f2 = torch.sum(f2, 1)
                delta_t = args.cme_dt
                f2 = 1 + (f2 - R) * args.cme_dt

            if torch.min(f2) < 0:
                f2[f2 <= 0] = args.epsilon

        log_Tp_t = torch.log(f2) + log_p_t

    return log_Tp_t, delta_t  # TP_t
